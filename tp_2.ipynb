{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Language-Image Pre-training\n",
    "\n",
    "In this lab session, we look at the representation given by CLIP. The CLIP representation gives the text encoder used in $\\mathrm{DALL}\\cdot\\mathrm{E} 2$. You can have a look at the original paper, here <https://arxiv.org/abs/2103.00020>.\n",
    "\n",
    "This lab session will require an access to a GPU. I recommend using Google Colab.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/vdeborto/lab-session-generative-modeling/blob/main/tp_2.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "We start by fetching the dataset FLickr 8k.\n",
    "It consists of $8000$ images each one of them paired with five different captions.\n",
    "\n",
    "To do so:\n",
    "* Create a Kaggle account (or sign-in if you have one) <https://www.kaggle.com/>\n",
    "* On your profile go to \"Account\".\n",
    "* In the \"API\" block, select \"Create New API Token\".\n",
    "* This will download a `.json` file with two keys `username` and `key`.\n",
    "* In the block below replace `KAGGLE_USERNAME` with the value associated with `username` and `KAGGLE_KEY` with the one associated with `key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (0.6.12)\n",
      "Requirement already satisfied: torchvision in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from timm) (0.12.0+cpu)\n",
      "Requirement already satisfied: torch>=1.7 in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from timm) (1.11.0+cpu)\n",
      "Requirement already satisfied: huggingface-hub in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from timm) (0.11.1)\n",
      "Requirement already satisfied: pyyaml in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from timm) (6.0)\n",
      "Requirement already satisfied: typing-extensions in /home/debortoli/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (4.4.0)\n",
      "Requirement already satisfied: tqdm in /home/debortoli/.local/lib/python3.10/site-packages (from huggingface-hub->timm) (4.64.1)\n",
      "Requirement already satisfied: filelock in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from huggingface-hub->timm) (3.8.2)\n",
      "Requirement already satisfied: requests in /home/debortoli/.local/lib/python3.10/site-packages (from huggingface-hub->timm) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/debortoli/.local/lib/python3.10/site-packages (from huggingface-hub->timm) (22.0)\n",
      "Requirement already satisfied: numpy in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from torchvision->timm) (1.22.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from torchvision->timm) (9.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/debortoli/.local/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/debortoli/.local/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (1.26.13)\n",
      "Requirement already satisfied: transformers in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (4.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from transformers) (2022.8.17)\n",
      "Requirement already satisfied: requests in /home/debortoli/.local/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/debortoli/.local/lib/python3.10/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/debortoli/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/debortoli/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/debortoli/.local/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/debortoli/.local/lib/python3.10/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/debortoli/anaconda3/envs/sandbox/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -u\n",
      "/bin/bash: line 1: kaggle: command not found\n",
      "unzip:  cannot find or open flickr8k.zip, flickr8k.zip.zip or flickr8k.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!pip install timm\n",
    "!pip install transformers\n",
    "!pip install kaggle --upgrade\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from tqdm.autonotebook import tqdm\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = \"vdeborto\"\n",
    "os.environ['KAGGLE_KEY'] = \"7f53adeaf4ef38210fba87c29ad7e447\"\n",
    "\n",
    "!kaggle datasets download - d adityajn105/flickr8k\n",
    "!unzip flickr8k.zip\n",
    "dataset = \"8k\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some preprocessing, utils and configurations\n",
    "\n",
    "Nothing to do for you here. In the first cell, we just apply some preprocessing to the data. In the second we store all the useful parameters of the models. In the third we write some utils functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mcaptions.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [id_ \u001b[39mfor\u001b[39;00m id_ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(df\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m5\u001b[39m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m)]\n\u001b[1;32m      3\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mcaptions.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"captions.txt\")\n",
    "df['id'] = [id_ for id_ in range(df.shape[0] // 5) for _ in range(5)]\n",
    "df.to_csv(\"captions.csv\", index=False)\n",
    "df = pd.read_csv(\"captions.csv\")\n",
    "image_path = \"/content/Images\"\n",
    "captions_path = \"/content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCFG\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     debug \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     image_path \u001b[39m=\u001b[39m image_path\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mCFG\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCFG\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     debug \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     image_path \u001b[39m=\u001b[39m image_path\n\u001b[1;32m      4\u001b[0m     captions_path \u001b[39m=\u001b[39m captions_path\n\u001b[1;32m      5\u001b[0m     batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_path' is not defined"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    image_path = image_path\n",
    "    captions_path = captions_path\n",
    "    batch_size = 32\n",
    "    num_workers = 2\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1\n",
    "    factor = 0.8\n",
    "    epochs = 4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_name = 'resnet50'\n",
    "    image_embedding = 2048\n",
    "    text_encoder_model = \"distilbert-base-uncased\"\n",
    "    text_embedding = 768\n",
    "    text_tokenizer = \"distilbert-base-uncased\"\n",
    "    max_length = 200\n",
    "\n",
    "    pretrained = True  # for both image encoder and text encoder\n",
    "    trainable = True  # for both image encoder and text encoder\n",
    "    temperature = 1.0\n",
    "\n",
    "    # image size\n",
    "    size = 224\n",
    "\n",
    "    # for projection head; used for both image and text encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256\n",
    "    dropout = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TALK ABOUT THE TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCLIPDataset\u001b[39;00m(torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset):\n\u001b[1;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, image_filenames, captions, tokenizer, transforms):\n\u001b[1;32m      3\u001b[0m         \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m        image_filenames and cpations must have the same length; so, if there are\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m        multiple captions for each image, the image_filenames must have repetitive\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m        file names \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m        \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=CFG.max_length\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "\n",
    "        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item['caption'] = self.captions[idx]\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text and image encoders\n",
    "\n",
    "For the image encoder we choose a `ResNet50` with size of the output channel $2048$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mImageEncoder\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m    Encode images to a fixed size vector\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m      7\u001b[0m         \u001b[39mself\u001b[39m, model_name\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mmodel_name, pretrained\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mpretrained, trainable\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mtrainable\n\u001b[1;32m      8\u001b[0m     ):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `DistilBERT` as the text encoder. For each sequence of tokens we add two extra tokens `CLS` and `SEP`. `CLS` corresponds to the start of the sentence and `SEP` to the end of the sentence. After the neural network transformation, each token is associated with a hidden representation of size $768$. We use the hidden representation of the **first** token as a representation for the **whole** sequence of input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTextEncoder\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model_name\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mtext_encoder_model, pretrained\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mpretrained, trainable\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mtrainable):\n\u001b[1;32m      3\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know write the projection head which consists into projecting the text encoding and image encoding in a space with similar dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mProjectionHead\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m      3\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m      4\u001b[0m         embedding_dim,\n\u001b[1;32m      5\u001b[0m         projection_dim\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mprojection_dim,\n\u001b[1;32m      6\u001b[0m         dropout\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mdropout\n\u001b[1;32m      7\u001b[0m     ):\n\u001b[1;32m      8\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=CFG.projection_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CLIP model\n",
    "\n",
    "Now, we are ready to write the core function defining CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCLIPModel\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m      3\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m      4\u001b[0m         temperature\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mtemperature,\n\u001b[1;32m      5\u001b[0m         image_embedding\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mimage_embedding,\n\u001b[1;32m      6\u001b[0m         text_embedding\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mtext_embedding,\n\u001b[1;32m      7\u001b[0m     ):\n\u001b[1;32m      8\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        image_embedding=CFG.image_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss = (images_loss + texts_loss) / 2.0  # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Let us define some helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_dfs():\n",
    "    dataframe = pd.read_csv(f\"{CFG.captions_path}/captions.csv\")\n",
    "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = dataframe[dataframe[\"id\"].isin(\n",
    "        train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = dataframe[dataframe[\"id\"].isin(\n",
    "        valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe\n",
    "\n",
    "\n",
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    transforms = get_transforms(mode=mode)\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"image\"].values,\n",
    "        dataframe[\"caption\"].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        num_workers=CFG.num_workers,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function (should take about 30 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter\n",
    "\n",
    "\n",
    "def valid_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "    return loss_meter\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_df, valid_df = make_train_valid_dfs()\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
    "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "\n",
    "\n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    params = [\n",
    "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
    "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
    "        {\"params\": itertools.chain(\n",
    "            model.image_projection.parameters(), model.text_projection.parameters()\n",
    "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
    "    )\n",
    "    step = \"epoch\"\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = valid_epoch(model, valid_loader)\n",
    "        \n",
    "        if valid_loss.avg < best_loss:\n",
    "            best_loss = valid_loss.avg\n",
    "            torch.save(model.state_dict(), \"best.pt\")\n",
    "            print(\"Saved Best Model!\")\n",
    "        \n",
    "        lr_scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's inference time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to get the image embedding of the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(valid_df, model_path):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "\n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
    "    model.eval()\n",
    "\n",
    "    valid_image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n",
    "            image_embeddings = model.image_projection(image_features)\n",
    "            valid_image_embeddings.append(image_embeddings)\n",
    "    return model, torch.cat(valid_image_embeddings)\n",
    "\n",
    "\n",
    "_, valid_df = make_train_valid_dfs()\n",
    "model, image_embeddings = get_image_embeddings(valid_df, \"best.pt\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to find matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    encoded_query = tokenizer([query])\n",
    "    batch = {\n",
    "        key: torch.tensor(values).to(CFG.device)\n",
    "        for key, values in encoded_query.items()\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        text_embeddings = model.text_projection(text_features)\n",
    "\n",
    "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
    "\n",
    "    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n",
    "    matches = [image_filenames[idx] for idx in indices[::5]]\n",
    "\n",
    "    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    for match, ax in zip(matches, axes.flatten()):\n",
    "        image = cv2.imread(f\"{CFG.image_path}/{match}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
